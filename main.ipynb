{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa6776e7-3611-4620-b3ed-40038049bc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (4.14.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (2.3.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (1.7.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (4.4.0)\n",
      "Requirement already satisfied: pyLDAvis in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 11)) (3.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from requests->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from requests->-r requirements.txt (line 2)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from requests->-r requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from requests->-r requirements.txt (line 2)) (2025.11.12)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 3)) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 3)) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from tqdm->-r requirements.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from nltk->-r requirements.txt (line 8)) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from nltk->-r requirements.txt (line 8)) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from nltk->-r requirements.txt (line 8)) (2025.11.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 9)) (1.16.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 9)) (3.6.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from gensim->-r requirements.txt (line 10)) (7.5.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from pyLDAvis->-r requirements.txt (line 11)) (3.1.6)\n",
      "Requirement already satisfied: numexpr in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from pyLDAvis->-r requirements.txt (line 11)) (2.14.1)\n",
      "Requirement already satisfied: funcy in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from pyLDAvis->-r requirements.txt (line 11)) (2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from pyLDAvis->-r requirements.txt (line 11)) (80.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from smart_open>=1.8.1->gensim->-r requirements.txt (line 10)) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\christianus yanuar\\miniconda3\\lib\\site-packages (from jinja2->pyLDAvis->-r requirements.txt (line 11)) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01cfdc5d-5be1-445c-8bb5-3a75531f7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dac4dd7e-59e9-447c-ad98-83eeb31bf684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6551b60ed0a74a8dba1e4bb29c8daa46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping Articles:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selesai. Total 8 artikel disimpan di contexts.json\n"
     ]
    }
   ],
   "source": [
    "def scrape_wikipedia_article(article_title):\n",
    "    \"\"\"Mengambil judul dan konten teks utama dari sebuah artikel Wikipedia.\"\"\"\n",
    "    url = f\"https://id.wikipedia.org/wiki/{article_title.replace(' ', '_')}\"\n",
    "    #Ini hanyalah contoh,ganti dengan URL yang anda inginkan\n",
    "    \n",
    "    try:\n",
    "        headers = {'User-Agent': 'Custom NLP Crawler 1.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status() \n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Selektor spesifik Wikipedia untuk konten utama\n",
    "        title = soup.find('h1', {'id': 'firstHeading'}).get_text(strip=True)\n",
    "        content_div = soup.find('div', {'id': 'mw-content-text'})\n",
    "        \n",
    "        paragraphs = content_div.find_all('p') if content_div else []\n",
    "        \n",
    "        full_text = []\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            # Menghilangkan referensi [1], [2], dll.\n",
    "            text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "            \n",
    "            if len(text) > 100: # Filter paragraf yang terlalu pendek\n",
    "                full_text.append(text)\n",
    "        \n",
    "        context = ' '.join(full_text)\n",
    "        context = re.sub(r'\\s+', ' ', context).strip()\n",
    "        \n",
    "        return title, context\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        # print(f\"Gagal mengambil URL {url}: {e}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        # print(f\"Error pemrosesan HTML {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "article_titles = [\n",
    "    \"Ekonomi Indonesia\", \"Politik Indonesia\", \"Budaya Indonesia\",\n",
    "    \"Sejarah Jakarta\", \"Sepak bola di Indonesia\", \"Gunung di Indonesia\",\n",
    "    \"Pariwisata di Bali\", \"Bahasa Indonesia\", \"Hewan endemik Indonesia\",\n",
    "    \"Presiden Indonesia\", \"Sistem pemerintahan Indonesia\"\n",
    "    \n",
    "]\n",
    "\n",
    "scraped_data = []\n",
    "for title in tqdm(article_titles, desc=\"Scraping Articles\"):\n",
    "    article_title, context = scrape_wikipedia_article(title)\n",
    "    \n",
    "    if context and len(context) > 500: # Minimum panjang konteks\n",
    "        scraped_data.append({\n",
    "            \"title\": article_title,\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "    \n",
    "    time.sleep(random.uniform(1.5, 3)) \n",
    "\n",
    "# Simpan ke JSON\n",
    "output_file = \"contexts.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(scraped_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "print(f\"\\nSelesai. Total {len(scraped_data)} artikel disimpan di {output_file}\")\n",
    "df = pd.DataFrame(scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19034633-1d4a-469d-9daa-48ff3d7bc820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data setelah preprocessing:\n",
      "                     title                                      clean_context\n",
      "0        Ekonomi Indonesia  ekonomiindonesiamerupakan ekonomi terbesar dia...\n",
      "1        Politik Indonesia  politik indonesiaadalah berlangsung dalam rang...\n",
      "2         Budaya Indonesia  kebudayaan indonesiaadalah seluruh kebudayaan ...\n",
      "3          Sejarah Jakarta  jakartaadalah ibu kota kota terbesarindonesia ...\n",
      "4  Sepak bola di Indonesia  sepak bolaadalah salah satu olahraga paling po...\n",
      "\n",
      "Hasil Pemodelan Topik (LDA):\n",
      "Topik #1: bahasa presiden wakil melayu konstitusi kepresidenan republik kata lembaga ris\n",
      "Topik #2: bola kebudayaan sunda pelabuhan nasional daerah kota budaya bangsa merupakan\n",
      "Topik #3: republik rakyat pemilihan presiden kekuasaan politik anggota umum daerah perwakilan\n",
      "Topik #4: ekonomi keuangan krisis mencapai publik anggaran kebijakan lebih hal sektor\n",
      "Topik #5: bali pulau sampai mereka asal tiongkok juga pelayaran sektor para\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "stop_words_id = [\n",
    "    'yang', 'dan', 'di', 'dari', 'itu', 'ini', 'adalah', 'dengan', 'sebagai', \n",
    "    'pada', 'untuk', 'tidak', 'atau', 'akan', 'telah', 'serta', 'namun', \n",
    "    # Tambahkan daftar stop words Indonesia yang lebih lengkap di sini\n",
    "]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercasing\n",
    "    text = text.lower()\n",
    "    # 2. non-alfanumerik\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # 3. Tokenisasi\n",
    "    tokens = text.split()\n",
    "    # 4. Hapus stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words_id and len(word) > 2]\n",
    "    # 5. Gabungkan kembali (untuk scikit-learn)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean_context'] = df['context'].apply(preprocess_text)\n",
    "print(\"Data setelah preprocessing:\")\n",
    "print(df[['title', 'clean_context']].head())\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=stop_words_id)\n",
    "dtm = vectorizer.fit_transform(df['clean_context'])\n",
    "\n",
    "#Pemodelan LDA\n",
    "n_topics = 5 # jumlah topik\n",
    "lda_model = LDA(n_components=n_topics, random_state=42, max_iter=10)\n",
    "lda_model.fit(dtm)\n",
    "\n",
    "# Fungsi untuk menampilkan kata-kata di setiap topik\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Topik #{topic_idx + 1}: \"\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        print(message)\n",
    "\n",
    "print(\"\\nHasil Pemodelan Topik (LDA):\")\n",
    "display_topics(lda_model, vectorizer.get_feature_names_out(), no_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b358b4e-4669-4577-905c-fe441559e914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasifikasi Otomatis Dokumen:\n",
      "                     title     dominant_topic_name  topic_probability\n",
      "0        Ekonomi Indonesia   Pariwisata & Geografi           0.939604\n",
      "1        Politik Indonesia           Budaya & Seni           0.937469\n",
      "2         Budaya Indonesia  Politik & Pemerintahan           0.931467\n",
      "3          Sejarah Jakarta  Politik & Pemerintahan           0.908029\n",
      "4  Sepak bola di Indonesia  Politik & Pemerintahan           0.903027\n",
      "5       Pariwisata di Bali     Bahasa & Linguistik           0.896757\n",
      "6         Bahasa Indonesia     Ekonomi & Finansial           0.885739\n",
      "7       Presiden Indonesia     Ekonomi & Finansial           0.894258\n"
     ]
    }
   ],
   "source": [
    "# Menetapkan Topik Dominan\n",
    "topic_results = lda_model.transform(dtm)\n",
    "# Cari indeks topik dengan probabilitas tertinggi di setiap dokumen\n",
    "df['dominant_topic_index'] = topic_results.argmax(axis=1)\n",
    "\n",
    "# Tambahkan probabilitas topik dominan\n",
    "df['topic_probability'] = topic_results.max(axis=1)\n",
    "\n",
    "# Asumsi: Anda sudah menginterpretasikan dan memberi nama pada topik (misalnya Topik 0 = Ekonomi)\n",
    "topic_names = {\n",
    "    0: \"Ekonomi & Finansial\", \n",
    "    1: \"Politik & Pemerintahan\", \n",
    "    2: \"Budaya & Seni\", \n",
    "    3: \"Pariwisata & Geografi\",\n",
    "    4: \"Bahasa & Linguistik\" \n",
    "}\n",
    "df['dominant_topic_name'] = df['dominant_topic_index'].map(topic_names)\n",
    "\n",
    "print(\"Klasifikasi Otomatis Dokumen:\")\n",
    "print(df[['title', 'dominant_topic_name', 'topic_probability']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88b1c4a1-f06a-4989-bc22-a0341a8fe618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "HASIL EVALUASI MATRIKS PENGUKURAN\n",
      "1. Metode Evaluasi: Coherence Score (C_v)\n",
      "2. Coherence Score (C_v) Model: 0.2631\n",
      "Interpretasi: Semakin tinggi nilainya (mendekati 1.0), semakin baik model Anda menemukan topik yang masuk akal.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 1. Tokenisasi ulang untuk format Gensim\n",
    "processed_texts = [doc.split() for doc in df['clean_context']]\n",
    "\n",
    "# 2. Membuat Dictionary dan Corpus (format yang dibutuhkan Gensim)\n",
    "dictionary = Dictionary(processed_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "# 3. Konversi model Scikit-learn LDA ke format Gensim LDA (atau latih ulang menggunakan Gensim)\n",
    "lda_gensim = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=n_topics,\n",
    "    random_state=42,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "# 4. Hitung Coherence Score (Metrik C_v adalah yang paling umum)\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda_gensim, \n",
    "    texts=processed_texts, \n",
    "    dictionary=dictionary, \n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "\n",
    "# Tampilkan Matriks Pengukuran\n",
    "print(\"=\"*50)\n",
    "print(\"HASIL EVALUASI MATRIKS PENGUKURAN\")\n",
    "print(f\"1. Metode Evaluasi: Coherence Score (C_v)\")\n",
    "print(f\"2. Coherence Score (C_v) Model: {coherence_score:.4f}\")\n",
    "print(f\"Interpretasi: Semakin tinggi nilainya (mendekati 1.0), semakin baik model Anda menemukan topik yang masuk akal.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05018ed2-0a26-4dc4-9620-b0698cd0d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Testing\n",
    "\n",
    "def preprocess_text_for_test(text):\n",
    "    return preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40eb0896-69c6-456d-a143-1631dff40a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HASIL INFERENSI MODEL LDA PADA DOKUMEN BARU\n",
      "============================================================\n",
      "--- [TEST 1] ---\n",
      "Input: Bank Indonesia menaikkan suku bunga acuan 25 basis poin untuk mengendalikan infl...\n",
      "Topik Ditemukan (Index): Topik #3\n",
      "Nama Topik Dominan: Pariwisata & Geografi\n",
      "Probabilitas Dominan: 0.7226\n",
      "\n",
      "Probabilitas Semua Topik:\n",
      "  - Ekonomi & Finansial: 0.0688\n",
      "  - Politik & Pemerintahan: 0.0715\n",
      "  - Budaya & Seni: 0.0685\n",
      "  - Pariwisata & Geografi: 0.7226\n",
      "  - Bahasa & Linguistik: 0.0686\n",
      "------------------------------------------------------------\n",
      "--- [TEST 2] ---\n",
      "Input: Dewan Perwakilan Rakyat (DPR) menyetujui Rancangan Undang-Undang baru setelah me...\n",
      "Topik Ditemukan (Index): Topik #2\n",
      "Nama Topik Dominan: Budaya & Seni\n",
      "Probabilitas Dominan: 0.7943\n",
      "\n",
      "Probabilitas Semua Topik:\n",
      "  - Ekonomi & Finansial: 0.0521\n",
      "  - Politik & Pemerintahan: 0.0512\n",
      "  - Budaya & Seni: 0.7943\n",
      "  - Pariwisata & Geografi: 0.0512\n",
      "  - Bahasa & Linguistik: 0.0511\n",
      "------------------------------------------------------------\n",
      "--- [TEST 3] ---\n",
      "Input: Pertunjukan wayang kulit dengan dalang ternama menarik ribuan penonton, membukti...\n",
      "Topik Ditemukan (Index): Topik #1\n",
      "Nama Topik Dominan: Politik & Pemerintahan\n",
      "Probabilitas Dominan: 0.7273\n",
      "\n",
      "Probabilitas Semua Topik:\n",
      "  - Ekonomi & Finansial: 0.0680\n",
      "  - Politik & Pemerintahan: 0.7273\n",
      "  - Budaya & Seni: 0.0683\n",
      "  - Pariwisata & Geografi: 0.0685\n",
      "  - Bahasa & Linguistik: 0.0680\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Teks input baru (harus relevan dengan salah satu topik yang Anda latih)\n",
    "new_texts = [\n",
    "    # Teks 1: Ekonomi (Prediksi: Topik 1)\n",
    "    \"Bank Indonesia menaikkan suku bunga acuan 25 basis poin untuk mengendalikan inflasi dan menstabilkan nilai tukar rupiah terhadap dolar Amerika.\", \n",
    "    \n",
    "    # Teks 2: Politik (Prediksi: Topik 2)\n",
    "    \"Dewan Perwakilan Rakyat (DPR) menyetujui Rancangan Undang-Undang baru setelah melalui proses voting yang panjang dan alot di sidang paripurna.\",\n",
    "    \n",
    "    # Teks 3: Budaya (Prediksi: Topik 3)\n",
    "    \"Pertunjukan wayang kulit dengan dalang ternama menarik ribuan penonton, membuktikan seni tradisi masih diminati generasi muda.\"\n",
    "]\n",
    "\n",
    "# Mapping nama topik (gunakan mapping dari Cell 4)\n",
    "topic_names = {\n",
    "    0: \"Ekonomi & Finansial\", \n",
    "    1: \"Politik & Pemerintahan\", \n",
    "    2: \"Budaya & Seni\", \n",
    "    3: \"Pariwisata & Geografi\",\n",
    "    4: \"Bahasa & Linguistik\" \n",
    "}\n",
    "n_topics = len(topic_names)\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"HASIL INFERENSI MODEL LDA PADA DOKUMEN BARU\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, text in enumerate(new_texts):\n",
    "    # 1. Preprocessing\n",
    "    clean_text = preprocess_text_for_test(text)\n",
    "    \n",
    "    # 2. Vektorisasi\n",
    "    dtm_new = vectorizer.transform([clean_text])\n",
    "    \n",
    "    # 3. Prediksi Probabilitas Topik\n",
    "    topic_probabilities = lda_model.transform(dtm_new)[0]\n",
    "    \n",
    "    # 4. Menemukan Topik Dominan\n",
    "    dominant_topic_index = np.argmax(topic_probabilities)\n",
    "    dominant_topic_prob = topic_probabilities[dominant_topic_index]\n",
    "    \n",
    "    print(f\"--- [TEST {i+1}] ---\")\n",
    "    print(f\"Input: {text[:80]}...\")\n",
    "    print(f\"Topik Ditemukan (Index): Topik #{dominant_topic_index}\")\n",
    "    print(f\"Nama Topik Dominan: {topic_names.get(dominant_topic_index, 'Tidak Dikenal')}\")\n",
    "    print(f\"Probabilitas Dominan: {dominant_topic_prob:.4f}\")\n",
    "    \n",
    "    # (Opsional) Tampilkan semua probabilitas\n",
    "    print(\"\\nProbabilitas Semua Topik:\")\n",
    "    for topic_idx in range(n_topics):\n",
    "        print(f\"  - {topic_names.get(topic_idx)}: {topic_probabilities[topic_idx]:.4f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703f8a35-e392-4cf0-ad43-40800ada1cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
